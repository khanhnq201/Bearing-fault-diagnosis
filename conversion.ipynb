{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7adc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1834943/3962559205.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('./outputs/models/test.pth', map_location=torch.device('cpu')) # Dùng map_location để đảm bảo chạy được trên CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã export thành công mô hình sang ./outputs/models/test.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src import models # Import kiến trúc model của bạn\n",
    "import cfg\n",
    "\n",
    "# --- Tải mô hình đã được tỉa và fine-tune ---\n",
    "# Đảm bảo bạn đã thực hiện bước prune.remove() để mô hình không còn mask\n",
    "model = models.Deeper1DCNN(input_channels=1, num_classes=3)\n",
    "# Giả sử bạn đã lưu state_dict sau khi remove mask\n",
    "checkpoint = torch.load('./outputs/models/test.pth', map_location=torch.device('cpu')) # Dùng map_location để đảm bảo chạy được trên CPU\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "model.cpu() # Chuyển về CPU trước khi export\n",
    "\n",
    "# --- Export sang ONNX ---\n",
    "onnx_file_path = \"./outputs/models/test.onnx\"\n",
    "dummy_input = torch.randn(1, 1, cfg.SAMPLE_LENGTH) # Batch size = 1\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_file_path,\n",
    "    input_names=[\"x\"], output_names=[\"logits\"],\n",
    "    dynamic_axes={\"x\": {0: \"N\", 2: \"L\"}, \"logits\": {0: \"N\"}},\n",
    "    opset_version=13, do_constant_folding=True\n",
    ")\n",
    "\n",
    "print(f\"Đã export thành công mô hình sang {onnx_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be89069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Importing: Record = '97', Key = 'DE'\n",
      "    [INFO] Label: Normal\n",
      "    [INFO] Size: 119\n",
      ">>> Importing: Record = '97', Key = 'FE'\n",
      "    [INFO] Label: Normal\n",
      "    [INFO] Size: 119\n",
      ">>> Importing: Record = '98', Key = 'DE'\n",
      "    [INFO] Label: Normal\n",
      "    [INFO] Size: 236\n",
      ">>> Importing: Record = '209', Key = 'FE'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '210', Key = 'DE'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '278', Key = 'DE'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '278', Key = 'FE'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '280', Key = 'DE'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '280', Key = 'BA'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '271', Key = 'DE'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '271', Key = 'FE'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '276', Key = 'BA'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '277', Key = 'FE'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '277', Key = 'BA'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '130', Key = 'DE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '131', Key = 'DE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '144', Key = 'DE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '144', Key = 'BA'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '145', Key = 'DE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '145', Key = 'FE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '145', Key = 'BA'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '156', Key = 'DE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '156', Key = 'FE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '310', Key = 'DE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '310', Key = 'FE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '311', Key = 'DE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '311', Key = 'FE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '313', Key = 'DE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '313', Key = 'FE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      "==================== Data for training ====================\n",
      "Class 0: 630 samples (23.64%)\n",
      "Class 1: 859 samples (32.23%)\n",
      "Class 2: 1176 samples (44.13%)\n",
      "==================================================\n",
      ">>> Importing: Record = '99', Key = 'DE'\n",
      "    [INFO] Label: Normal\n",
      "    [INFO] Size: 236\n",
      ">>> Importing: Record = '99', Key = 'FE'\n",
      "    [INFO] Label: Normal\n",
      "    [INFO] Size: 236\n",
      ">>> Importing: Record = '211', Key = 'DE'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '279', Key = 'DE'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '274', Key = 'FE'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '272', Key = 'DE'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '272', Key = 'FE'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '272', Key = 'BA'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '132', Key = 'DE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '146', Key = 'DE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '146', Key = 'FE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '146', Key = 'BA'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '159', Key = 'DE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '312', Key = 'DE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '312', Key = 'FE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '315', Key = 'DE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 58\n",
      "==================== Data for validating ====================\n",
      "Class 0: 472 samples (36.39%)\n",
      "Class 1: 354 samples (27.29%)\n",
      "Class 2: 471 samples (36.31%)\n",
      "==================================================\n",
      ">>> Importing: Record = '100', Key = 'DE'\n",
      "    [INFO] Label: Normal\n",
      "    [INFO] Size: 237\n",
      ">>> Importing: Record = '100', Key = 'FE'\n",
      "    [INFO] Label: Normal\n",
      "    [INFO] Size: 237\n",
      ">>> Importing: Record = '212', Key = 'DE'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '281', Key = 'DE'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '275', Key = 'FE'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '273', Key = 'DE'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '273', Key = 'FE'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '273', Key = 'BA'\n",
      "    [INFO] Label: IR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '133', Key = 'DE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '147', Key = 'DE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '147', Key = 'FE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '147', Key = 'BA'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '160', Key = 'DE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 59\n",
      ">>> Importing: Record = '317', Key = 'DE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 58\n",
      ">>> Importing: Record = '317', Key = 'FE'\n",
      "    [INFO] Label: OR\n",
      "    [INFO] Size: 58\n",
      "==================== Data for testing ====================\n",
      "Class 0: 474 samples (38.26%)\n",
      "Class 1: 354 samples (28.57%)\n",
      "Class 2: 411 samples (33.17%)\n",
      "==================== Training data after imbalancing ====================\n",
      "Class 0: 630 samples (91.04%)\n",
      "Class 1: 31 samples (4.48%)\n",
      "Class 2: 31 samples (4.48%)\n",
      "==================== SMOTING DATA ====================\n",
      "Class 0: 630 samples (33.33%)\n",
      "Class 1: 630 samples (33.33%)\n",
      "Class 2: 630 samples (33.33%)\n",
      "\n",
      "==================================================\n",
      "DATA DISTRIBUTION SUMMARY\n",
      "==================================================\n",
      "Training samples:     1890\n",
      "Validation samples:   1297\n",
      "Test samples:         1239\n",
      "Total samples:        4426\n"
     ]
    }
   ],
   "source": [
    "if cfg.IMBALANCED_MODE == True: \n",
    "    X_train, Y_train, X_val, Y_val, X_test, Y_test = data_loader.data_import(cfg.OVERLAPPING_RATIO,cfg.BASE_PATH,cfg.SAMPLE_LENGTH, cfg.PREPROCESSING)\n",
    "    X_train_imbalanced, Y_train_imbalanced = data_loader.create_imbalanced_data(X_train, Y_train, cfg.IMBALANCED_RATIO, cfg.NUM_CLASSES)\n",
    "    #X_train_normalized, X_val_normalized, X_test_normalized = data_loader.normalize_data(X_train_imbalanced, X_val, X_test)\n",
    "    X_train_smoted, Y_train_smoted = data_loader.apply_smote(X_train_imbalanced, Y_train_imbalanced, cfg.RANDOM_STATE, cfg.SAMPLE_LENGTH)\n",
    "    train_loader, val_loader, test_loader = data_loader.create_dataloaders(X_train_smoted, Y_train_smoted,\n",
    "                                                                            X_val, Y_val,\n",
    "                                                                            X_test, Y_test,\n",
    "                                                                            cfg.SAMPLE_LENGTH, cfg.CNN1D_INPUT, cfg.BATCH_SIZE, cfg.RANDOM_STATE)\n",
    "else:\n",
    "    X_train, Y_train, X_val, Y_val, X_test, Y_test = data_loader.data_import(cfg.OVERLAPPING_RATIO,cfg.BASE_PATH,cfg.SAMPLE_LENGTH,cfg.PREPROCESSING)\n",
    "    X_train_normalized, X_val_normalized, X_test_normalized = data_loader.normalize_data(X_train, X_val, X_test)\n",
    "    train_loader, val_loader, test_loader = data_loader.create_dataloaders(X_train, Y_train,\n",
    "                                                                            X_val, Y_val,\n",
    "                                                                            X_test, Y_test,\n",
    "                                                                            cfg.SAMPLE_LENGTH, cfg.CNN1D_INPUT, cfg.BATCH_SIZE, cfg.RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d3bf3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu 640 mẫu dữ liệu hiệu chỉnh vào calibration_data.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# train_loader = ... (tải train_loader của bạn)\n",
    "calibration_data = []\n",
    "num_calibration_batches = 5 # Ví dụ: 5 batches\n",
    "\n",
    "for i, (data, target) in enumerate(train_loader):\n",
    "    if i >= num_calibration_batches:\n",
    "        break\n",
    "    calibration_data.append(data.numpy())\n",
    "\n",
    "# Gộp các batch lại và lưu\n",
    "calibration_data = np.concatenate(calibration_data, axis=0)\n",
    "np.save(\"calibration_data.npy\", calibration_data)\n",
    "\n",
    "print(f\"Đã lưu {calibration_data.shape[0]} mẫu dữ liệu hiệu chỉnh vào calibration_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7003b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu quá trình trích xuất dữ liệu từ DataLoader...\n",
      "Đã trích xuất xong!\n",
      "Kích thước mảng features cuối cùng: (1239, 1, 2048)\n",
      "Kích thước mảng labels cuối cùng: (1239,)\n",
      "Đã lưu dữ liệu thành công vào file 'features_data.npy' và 'labels_data.npy'\n",
      "\n",
      "Kiểm tra lại file đã lưu:\n",
      "Kích thước features đã tải: (1239, 1, 2048)\n",
      "Kích thước labels đã tải: (1239,)\n",
      "Kiểm tra thành công, dữ liệu khớp!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Bắt đầu quá trình trích xuất dữ liệu từ DataLoader...\")\n",
    "\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# Vô hiệu hóa tính toán gradient để tăng tốc độ và giảm bộ nhớ\n",
    "with torch.no_grad():\n",
    "    for data_batch, labels_batch in test_loader:\n",
    "        # Chuyển tensor về CPU (nếu đang dùng GPU) rồi sang NumPy\n",
    "        # và thêm vào danh sách\n",
    "        all_features.append(data_batch.cpu().numpy())\n",
    "        all_labels.append(labels_batch.cpu().numpy())\n",
    "\n",
    "print(\"Đã trích xuất xong!\")\n",
    "\n",
    "# Bước 4: Nối các batch lại thành một mảng NumPy lớn duy nhất\n",
    "# np.concatenate sẽ ghép các mảng trong list theo trục đã cho (mặc định là 0)\n",
    "final_features_array = np.concatenate(all_features, axis=0)\n",
    "final_labels_array = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "print(f\"Kích thước mảng features cuối cùng: {final_features_array.shape}\")\n",
    "print(f\"Kích thước mảng labels cuối cùng: {final_labels_array.shape}\")\n",
    "\n",
    "# Bước 5: Lưu các mảng thành file .npy\n",
    "# Nên lưu features và labels thành 2 file riêng biệt\n",
    "np.save('X_test.npy', final_features_array)\n",
    "np.save('Y_test.npy', final_labels_array)\n",
    "\n",
    "print(\"Đã lưu dữ liệu thành công vào file 'features_data.npy' và 'labels_data.npy'\")\n",
    "\n",
    "# --- (Tùy chọn) Kiểm tra lại bằng cách tải file ---\n",
    "print(\"\\nKiểm tra lại file đã lưu:\")\n",
    "loaded_features = np.load('X_test.npy')\n",
    "loaded_labels = np.load('Y_test.npy')\n",
    "\n",
    "print(f\"Kích thước features đã tải: {loaded_features.shape}\")\n",
    "print(f\"Kích thước labels đã tải: {loaded_labels.shape}\")\n",
    "\n",
    "# Kiểm tra xem dữ liệu có khớp không\n",
    "assert np.array_equal(final_features_array, loaded_features)\n",
    "assert np.array_equal(final_labels_array, loaded_labels)\n",
    "print(\"Kiểm tra thành công, dữ liệu khớp!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ad7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "khanh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
